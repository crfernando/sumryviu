{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/B005SUHPO6.csv\n",
      "../data/B0090AAOUW.csv\n",
      "../data/B00KROF20M.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('../data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18688 entries, 0 to 18687\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      18688 non-null  int64  \n",
      " 1   reviewerID      18688 non-null  object \n",
      " 2   asin            18688 non-null  object \n",
      " 3   reviewerName    18685 non-null  object \n",
      " 4   helpful         18688 non-null  object \n",
      " 5   reviewText      18684 non-null  object \n",
      " 6   overall         18688 non-null  float64\n",
      " 7   summary         18688 non-null  object \n",
      " 8   unixReviewTime  18688 non-null  int64  \n",
      " 9   reviewTime      18688 non-null  object \n",
      "dtypes: float64(1), int64(2), object(7)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/B005SUHPO6.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18688 entries, 0 to 18687\n",
      "Data columns (total 1 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   reviewtext  18684 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 146.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.columns = map(str.lower, data.columns)\n",
    "data_reviews = data[['reviewtext']]\n",
    "data_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text_sample = data_reviews.sample(1000).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm disappointed that the screen protector bubbles. I should have sent it back, but we live out of the country. Hopefully next time it will fit almost like a water tight skin, like my first one.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text_sample[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    for text in texts:\n",
    "        yield(simple_preprocess(str(text),deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text_token = list(tokenize(review_text_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm disappointed that the screen protector bubbles. I should have sent it back, but we live out of the country. Hopefully next time it will fit almost like a water tight skin, like my first one.\"]\n",
      "['disappointed', 'that', 'the', 'screen', 'protector', 'bubbles', 'should', 'have', 'sent', 'it', 'back', 'but', 'we', 'live', 'out', 'of', 'the', 'country', 'hopefully', 'next', 'time', 'it', 'will', 'fit', 'almost', 'like', 'water', 'tight', 'skin', 'like', 'my', 'first', 'one']\n"
     ]
    }
   ],
   "source": [
    "print(review_text_sample[6])\n",
    "print(review_text_token[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in text if word not in stop_words] for text in texts]\n",
    "\n",
    "review_text_token_nostops = remove_stopwords(review_text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disappointed', 'screen', 'protector', 'bubbles', 'sent', 'back', 'live', 'country', 'hopefully', 'next', 'time', 'fit', 'almost', 'like', 'water', 'tight', 'skin', 'like', 'first', 'one']\n"
     ]
    }
   ],
   "source": [
    "print(review_text_token_nostops[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Bigrams and trigrams \n",
    "def make_bigram(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    \n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "review_text_bigrams = make_bigram(review_text_token_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disappointed', 'screen', 'protector', 'bubbles', 'sent', 'back', 'live', 'country', 'hopefully', 'next', 'time', 'fit', 'almost', 'like', 'water', 'tight', 'skin', 'like', 'first', 'one']\n"
     ]
    }
   ],
   "source": [
    "print(review_text_bigrams[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    \n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(' '.join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = lemmatization(review_text_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disappoint', 'screen', 'protector', 'bubble', 'send', 'back', 'live', 'country', 'hopefully', 'next', 'time', 'fit', 'almost', 'water', 'tight', 'skin', 'first']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20720\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(data_lemmatized)\n",
    "print(dictionary.num_nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'case'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in data_lemmatized]\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build LDA model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# # Fit LDA model on the dataset\n",
    "# ntopics = 3\n",
    "# lda = gensim.models.ldamodel.LdaModel # Instantiate LDA model\n",
    "# ldamodel = lda(corpus=corpus, id2word=dictionary, num_topics=ntopics, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# # Assigns the topics to the documents in corpus\n",
    "# lda_corpus = ldamodel[corpus]\n",
    "# # [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# # Compute Perplexity\n",
    "# perplexity_lda = ldamodel.log_perplexity(corpus)\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model=ldamodel, texts=data_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "# print('\\nPerplexity: ', perplexity_lda)  # a measure of how good the model is. lower the better.\n",
    "# print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize the topics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import pyLDAvis.gensim\n",
    "# import pyLDAvis\n",
    "# import warnings\n",
    "\n",
    "# # warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# pyLDAvis.enable_notebook()\n",
    "# lda_visual = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "# lda_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the optimal number of topics for LDA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# num_topics = list(range(16)[1:])\n",
    "# num_keywords = 10\n",
    "# chunk_size = len(corpus)\n",
    "\n",
    "# ldamodel_list = {}\n",
    "# ldamodel_coherences = {}\n",
    "# lda_topics = {}\n",
    "# lda = gensim.models.ldamodel.LdaModel # Instantiate LDA model\n",
    "# for i in num_topics:\n",
    "#     ldamodel_list[i] = lda(corpus=corpus, id2word=dictionary, num_topics=ntopics, update_every=1, chunksize=100, passes=10, alpha='auto', random_state=42, per_word_topics=True)\n",
    "#     shown_topics = ldamodel_list[i].show_topics(num_topics=num_topics, num_words=num_keywords, formatted=False)\n",
    "#     lda_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]\n",
    "#     ldamodel_coherences = [CoherenceModel(model=ldamodel_list[i], texts=corpus, dictionary=dictionary, coherence='c_v').get_coherence() for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence(corpus, dictionary, texts, chunk_size, topics_range, nkeywords):\n",
    "    model_list = {}\n",
    "    model_coherence = {}\n",
    "    model_topics = {}\n",
    "#     topics_range = list(range(start, limit, step)) # create number of topics range list\n",
    "    lda = gensim.models.ldamodel.LdaModel # instantiate LDA model\n",
    "    \n",
    "    for i in range(len(topics_range)):\n",
    "        num_topics = topics_range[i]\n",
    "        model_list[i] = lda(corpus=corpus, id2word=dictionary, num_topics=num_topics, update_every=1, chunksize=chunk_size, passes=10, alpha='auto', random_state=42, per_word_topics=True)\n",
    "        coherencemodel = CoherenceModel(model=model_list[i], texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        model_coherence[i] = coherencemodel.get_coherence()\n",
    "        shown_topics = model_list[i].show_topics(num_topics=num_topics, num_words=nkeywords, formatted=False)\n",
    "        model_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]\n",
    "    \n",
    "    return model_list, model_coherence, model_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_similarity(topic_1, topic_2):\n",
    "    \"\"\"\n",
    "    Derives the Jaccard similarity of two topics\n",
    "\n",
    "    Jaccard similarity:\n",
    "    - A statistic used for comparing the similarity and diversity of sample sets\n",
    "    - J(A,B) = (A ∩ B)/(A ∪ B)\n",
    "    - Goal is low Jaccard scores for coverage of the diverse elements\n",
    "    \"\"\"\n",
    "    intersection = set(topic_1).intersection(set(topic_2))\n",
    "    union = set(topic_1).union(set(topic_2))\n",
    "                    \n",
    "    return float(len(intersection))/float(len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_stability(topics, topics_range):\n",
    "    \"\"\"\n",
    "    derive the mean stability across topics by considering the next topic\n",
    "    \"\"\"\n",
    "    stability_scores = {}\n",
    "    mean_stabilities = {}\n",
    "\n",
    "    for i in range(0, len(topics_range)-1):\n",
    "        score = []\n",
    "        for t1, topic1 in enumerate(model_topics[i]):\n",
    "            jaccard_score = []\n",
    "            for t2, topic2 in enumerate(model_topics[i+1]):\n",
    "                jaccard_score.append(compute_jaccard_similarity(topic1, topic2))\n",
    "\n",
    "            score.append(jaccard_score)\n",
    "\n",
    "        stability_scores[i] = score\n",
    "        mean_stabilities[i] = np.array(stability_scores[i]).mean()\n",
    "    \n",
    "    return mean_stabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_start=2\n",
    "n_limit=35\n",
    "n_step=3\n",
    "n_words = 10\n",
    "topics_range = list(range(n_start, n_limit, n_step)) # create number of topics range list\n",
    "chunk_size = len(corpus)\n",
    "\n",
    "model_list, coherence_values, model_topics = compute_coherence(corpus=corpus, dictionary=dictionary, \n",
    "                                                               texts=data_lemmatized, chunk_size=chunk_size, \n",
    "                                                               topics_range=topics_range, nkeywords=n_words)\n",
    "\n",
    "mean_stabilities = compute_mean_stability(topics=model_topics, topics_range=topics_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_coherence_stability = []\n",
    "y_coherence_values = []\n",
    "y_mean_stabilities = []\n",
    "for i in range(0, len(topics_range)-1):\n",
    "    diff_coherence_stability.append(coherence_values[i] - mean_stabilities[i])\n",
    "    y_coherence_values.append(coherence_values[i])\n",
    "    y_mean_stabilities.append(mean_stabilities[i])\n",
    "    \n",
    "#     print('Coherence value: {} | Stability(mean): {} | {}'.format(round(coherence_values[i], 2), round(mean_stabilities[i], 2), round(diff_coherence_stability[i], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derive the ideal number of topics roughly through the difference between the coherence and stability per number of topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_diff_val = max(diff_coherence_stability)\n",
    "max_diff_idx = diff_coherence_stability.index(max_diff_val)\n",
    "ideal_num_topics = topics_range[max_diff_idx]\n",
    "\n",
    "print('Ideal number of topics for model selection is {}'.format(ideal_num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 5))\n",
    "x_topics_range = topics_range[:-1]\n",
    "\n",
    "plt.plot(x_topics_range, y_coherence_values, label = 'Topic Coherence')\n",
    "plt.plot(x_topics_range, y_mean_stabilities, label = 'Average Topic Overlap')\n",
    "plt.axvline(x = ideal_num_topics, label='Ideal Number of Topics', color='black', linestyle = ':', linewidth=1.2)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Metric Level')\n",
    "plt.title('Ideal number of topics for model selection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References\n",
    "- https://stackoverflow.com/questions/32313062/what-is-the-best-way-to-obtain-the-optimal-number-of-topics-for-a-lda-model-usin\n",
    "- https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "- https://www.kaggle.com/madhavi11089/topicmodelling-with-lda\n",
    "- https://github.com/raaga500/YTshared/blob/master/V4_TopicModelling_4.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
